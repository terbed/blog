{
  
    
        "post0": {
            "title": "Understanding weighted Cross Entropy loss and its effects",
            "content": "Problem description: . we have a multiclass classification problem | the class imbalance is very large -&gt; loss class weights are applied | in inference time the occurrence of a class (debris) is large -&gt; we want to classify rare classes in &quot;large noise&quot; | the rare classes have large amount of false positive predictions from the numerous debris class -&gt; we should punish this during training | . cross entropy loss: $$ - sum_{c=1}^{N}y_c log(p_c)$$ where N is the number of classes and c iterates through each class; $y$ is a binary label $[0, 1]$ and $p$ is the predicted class probability. We can create a probability value for each class with the application of softmax function: . $$ p_i = frac{e^{x_i}}{ sum_{c=1}^{N}e^{x_c}} $$ This function exponentially highlights the class with the largest assigned value and suppresses the other ones. At this point, information from the predictions of the other classes appear in the calculated probability value. If there was a high value assigned to another class that will suppress the current class probability. . If we have an imbalanced dataset we can apply class weights to correct the bias: $$ - sum_{c=1}^{N} w_c y_c log(p_c)$$ . Note that using LogSoftmax in the output layer and NLLLoss (negative log likelihood loss) is equivalent with cross entropy loss. The output layer of the NN is LogSoftmax: $$ LogSoftmax(x) = log frac{e^{x_c}}{ sum_{c=1}^{N} e^{x_c}} in [- inf, 0] $$ . where $x in mathcal{R}^{N}$ is the raw output of the NN; N is the number of classes. . $$ NLLLoss(x) = - sum_{c=1}^{N}y_c x_c $$ . for the sake of simplicity we will stick to the cross entropy formulation (although our implementation use this scheme). . We have 4 classes with the following sample distribution and assigned weights: . class1 (260) -&gt; 0.8 | class2 (10) -&gt; 26 | class3 (56) -&gt; 4 | class4 (500)-&gt; 0.4 | . The first 3 classes are rare objects and the fourth is the numerous debris class. . import numpy as np . class_weights = np.array([0.8, 26, 4, 0.4]) . def softmax(x: np.array): return np.exp(x)/np.sum(np.exp(x)) # CrossEntropyLoss def cross_entropy_loss(p: np.array, y: int, w: np.array): &quot;&quot;&quot; :param p: class probabilities :param y: ground truth class index :param w: class weights :return: &quot;&quot;&quot; return -1 * np.log(p[y]) * w[y] . # class 1 predicted correctly with large confidence y = 0 x = [10, 0.1, 0.1, 0.1] p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [9.99849499e-01 5.01671307e-05 5.01671307e-05 5.01671307e-05] 0.00012041017484880806 . np.sum(p) . 1.0 . # class 2 predicted correctly with large confidence y = 1 x = [0.10, 10, 0.1, 0.1] p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [5.01671307e-05 9.99849499e-01 5.01671307e-05 5.01671307e-05] 0.003913330682586261 . # class 3 predicted correctly with large confidence y = 2 x = [0.10, 0.10, 10, 0.1] p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [5.01671307e-05 5.01671307e-05 9.99849499e-01 5.01671307e-05] 0.0006020508742440402 . # class 4 predicted correctly with large confidence y = 3 x = [0.10, 0.10, 0.10, 10] p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [5.01671307e-05 5.01671307e-05 5.01671307e-05 9.99849499e-01] 6.020508742440403e-05 . The trivial cases are clear: for a rare class a larger weight is assigned -&gt; the network update for that class is scarce but its extent is large. . y = 0 # GT x = [10, 8, 0.1, 0.1] # Network raw output p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [8.80719233e-01 1.19192387e-01 4.41898075e-05 4.41898075e-05] 0.10161311565097696 . y = 0 # GT x = [10, 0.1, 0.1, 0.8] # Network raw output p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [9.99798652e-01 5.01645795e-05 5.01645795e-05 1.01019058e-04] 0.00016109479196323826 . y = 0 # GT x = [10, 8, 5, 0.1] # Network raw output p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [8.75562129e-01 1.18494449e-01 5.89949122e-03 4.39310514e-05] 0.10631133259521273 . x = [10, 8, 5, 5] # Network raw output p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [0.87046507 0.11780464 0.00586515 0.00586515] 0.11098211900127541 . x = [10, 9, 9, 9] # Network raw output p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [0.47536689 0.1748777 0.1748777 0.1748777 ] 0.5949347045029433 . The critical case for us is when a debris is classified to an another class. . y = 3 x = [10, 0.1, 0.1, 5] # Network raw output p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [9.93208148e-01 4.98339031e-05 4.98339031e-05 6.69218386e-03] 2.0027260085049603 . x = [10, 0.1, 0.1, 0.1] # Network raw output p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [9.99849499e-01 5.01671307e-05 5.01671307e-05 5.01671307e-05] 3.9600602050874247 . Ideas . To increase the punishment when a debris is wrongly classified we could play with the assigned weights. We could change the weights generated from the sample distribution to a custom value. . class_weights[3] = 1 x = [10, 0.1, 0.1, 0.1] # Network raw output p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [9.99849499e-01 5.01671307e-05 5.01671307e-05 5.01671307e-05] 9.900150512718561 . class_weights[3] = 0.8 x = [10, 0.1, 0.1, 0.1] # Network raw output p = softmax(x) l = cross_entropy_loss(p, y, class_weights) print(p) print(l) . [9.99849499e-01 5.01671307e-05 5.01671307e-05 5.01671307e-05] 7.9201204101748495 . The pitfalls of this is that the number of false negative predictions from the rare classes might increase, so it might tend to classify samples from rare class to the debris class. Thus, the weights must be carefully chosen. .",
            "url": "https://terbed.github.io/blog/2022/09/28/Understanding-NLLLoss.html",
            "relUrl": "/2022/09/28/Understanding-NLLLoss.html",
            "date": " • Sep 28, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello there! My name is Dániel Terbe. I live in Budapest, Hungary. In this page, I collect and summarize my milestones and achievements. We can look at this as my CV. . Experience . AI consultant &amp; Developer – Self-Employment . August 2022 – present . TongueScanner In Traditional Chinese Medicine (TCM) the tongue is a very important diagnostic tool. The tongue is a reflection of the internal organs and their functions and the mirror of the body’s health. | TongueScanner is an AI-based tongue analysis tool that can help TCM practitioners to diagnose their patients more accurately and efficiently. | My role is to research and develop the AI models for the application. | . | . June 2022 – September 2022 . contract with Daxire FinTech startup Application of machine learning and deep learning to the financial industry, focusing on crypto currencies. | Role: data scientist | . | . October 2020 – January 2021 . contract with the University of Szeged (SZTE), Hungary The project is about improving impaired speech (of dysarthric speakers) with deep learning tools, more precisely with Generative Adversarial Networks (GANs). | Lately, we experiment with sequence-to-sequence, transformer based voice conversion (VC) models. | . | . Developer &amp; Researcher – Institute for Computer Science and Control (SZTAKI), Computational Optical Sensing and Processing Laboratory, Budapest, Hungary . August 2017 – present . With an average camera, we are able to detect the subtle color changes in the skin that is associated with the cardiac-cycle, thus we can measure pulse-rate remotely. The name of this field is Remote Photoplethysmography (rPPG) which was my first topic here. | My tasks were to implement and test existing classical algorithms and deep neural network architectures from literature (python, pytorch, matlab), workout new ones and develop real-time software for (resource limited) embedded system environment (c++, Qt, raspberry pi, nvidia jetson nano). | I developed an iphone application (swift frontend and c++ backend) which can measure pulse from face or palm with front or rear camera. It is freely accessible in the apple appsotre: Remote Pulse App | Since the summer of 2020, I started to work in the field of Digital Holographic Microscopy (DHM) which operates with coherent light (eg. laser) instead of incoherent light like conventional microscopy. My task is to implement and train deep neural networks which are able to recover phase for a hologram image and generate a brightfield microscope image from a single hologram using generative adversarial networks (GANs). | Additionally, I work on neural-network based hologram focusing and object detection/counting/classification/segmentation. | . Research Intern – Turbine Ltd. . March 2016 – June 2016 . Turbine’s goal is to design effective cancer combination therapies using artificial intelligence and computer simulations. . My task was to explore machine learning (deep learning) applications in the field of computational biology and present it to the team leader. | . Student Researcher – Institute of Experimental Medicine (KOKI), Computational Neuroscience Workgroup, Budapest, Hungary . September 2015 – March 2022 . Statistical inference of neuron model parameters with computer simulations and Bayesian statistics tools to deliver richer information than simple point estimates (fitting). | I presented this work on Scientific Students’ Associations in Eötvös Loránd University and qualified for National Conference. | The source code (and thesis) can be found on GitHub: https://github.com/terbed/parameter-inference | . Publications . Terbe Dániel, László Orzó, and Ákos Zarándy. “Deep-learning-based bright-field image generation from a single hologram using an unpaired dataset.” Optics Letters 46.22 (2021): 5567-5570. . Nagy, Á., Földesy, P., Jánoki, I., Terbe, D., Siket, M., Szabó, M., … &amp; Zarándy, Á. (2021). Continuous camera-based premature-infant monitoring algorithms for NICU. Applied Sciences, 11(16), 7215. . Conferences . Terbe, D., Tóth, L. &amp; Ivaskó, L., 2022. Hangkorverzió alkalmazása dysarthriás betegek beszédminőségének javítására In XVIII. Magyar Számítógépes Nyelvészeti Konferencia. pp. 161–173. . In 2020 IEEE International Symposium on Circuits and Systems (ISCAS) (pp. 1-5). IEEE. Zarándy, Á., Földesy, P., Nagy, Á., Jánoki, I., Terbe, D., Siket, M., Szabó, M. and Varga, J., 2020, October. Multi-Level Optimization for Enabling Life Critical Visual Inspections of Infants in Resource Limited Environment. . KÉPAF 2019 (Debrecen, Hungary); The 12th Conference of Hungarian Association for Image Processing and Pattern Recognition Terbe, Dániel, and Ákos Zarándy. “Remote camera based heart rate estimation.” . CNNA 2018 (Budapest, Hungary); The 16th International Workshop on Cellular Nanoscale Networks and their Applications Terbe, Dániel, and Ákos Zarándy. “Remote camera based measurement of human vital signs.” CNNA 2018; The 16th International Workshop on Cellular Nanoscale Networks and their Applications. VDE, 2018. . Education . BSc. Physics with Theoretical Physics specialization in Eötvös Loránd University Budapest, Hungary . From 2013 to 2017 . My thesis work was about statistical inference of biophysical neuron model parameters. . MSc. Info-Bionics Engineering with specialization in Bionic Interfaces and Integrated Structures in Pázmány Péter Catholic University, Budapest, Hungary . From 2017 to 2020 . The title of my thesis work: Remote camera-based pulse estimation using deep learning tools which can be found here on GitHub. . Other . Summer school . Eastern European Machine Learning summer school, organised in Virtual Budapest between 7-15th of July, 2021. . Remote Pulse iphone App . I developed an Iphone application that can measure pulse remotely from face or palm. It is freely available in the apple appstore. The core algorithm is written in c++ and the UI is in swift. . Hobby . I like winter sports and mountain biking or hiking in nature. In my free time I play music, sing, read books, do yoga and meditate regularly. . Contact me . Currently, I live in Budapest, Hungary. email: terbed@gmail.com .",
          "url": "https://terbed.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://terbed.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}